{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "098fad5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from captum.attr import IntegratedGradients\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "84625bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary to be here since the when loading the models joblid tries to import  __main__.to_float32_fn it wouldn't find it here\n",
    "# this happens because the MLP model pipeline depdends on this function\n",
    "def to_float32_fn(X):\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "#same reason as above\n",
    "class MLP_IG(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.LazyLinear(hidden_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.out = nn.Linear(16, output_dim)\n",
    "\n",
    "        # # better initialization for IG smoothness\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) and not isinstance(m, nn.LazyLinear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        logits = self.out(x)  # no softmax\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7a8413ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_dir, dataset_id):\n",
    "    models = {\n",
    "        \"lr\": joblib.load(f\"{model_dir}/lr_{dataset_id}.joblib\"),\n",
    "        \"rf\": joblib.load(f\"{model_dir}/rf_{dataset_id}.joblib\"),\n",
    "        \"mlp\": joblib.load(f\"{model_dir}/mlp_{dataset_id}.joblib\")\n",
    "    }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "7ac99005",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_with_ig(model_pipeline, X_explain, target_class=1):\n",
    "    \"\"\"\n",
    "    Compute Integrated Gradients for a PyTorch model wrapped in a sklearn or skorch pipeline.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    model_pipeline : sklearn.pipeline.Pipeline or PyTorch nn.Module\n",
    "        If Pipeline, the last step must be a PyTorch model (skorch NeuralNetClassifier or nn.Module)\n",
    "    X_explain : pd.DataFrame or np.ndarray\n",
    "        The raw input data to explain (untransformed)\n",
    "    target_class : int\n",
    "        The target class index for which to compute attributions\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    np.ndarray\n",
    "        Integrated Gradients attributions (shape: n_samples x n_features)\n",
    "    \"\"\"\n",
    "\n",
    "    # If it is a Pipeline, separate preprocessing and model\n",
    "    if hasattr(model_pipeline, 'named_steps'):\n",
    "        preprocess = model_pipeline.named_steps['preprocess']\n",
    "        X_trans = preprocess.transform(X_explain)\n",
    "        # convert sparse to dense if necessary\n",
    "        if hasattr(X_trans, \"toarray\"):\n",
    "            X_trans = X_trans.toarray()\n",
    "        # get the model (unwrap skorch if necessary)\n",
    "        model = model_pipeline.named_steps['clf']\n",
    "    else:\n",
    "        # assume X_explain is already transformed\n",
    "        X_trans = X_explain\n",
    "        model = model_pipeline\n",
    "\n",
    "    # unwrap skorch model to get raw nn.Module\n",
    "    if hasattr(model, \"module_\"):\n",
    "        model = model.module_\n",
    "\n",
    "    model.eval()\n",
    "    ig = IntegratedGradients(model)\n",
    "\n",
    "    attributions = []\n",
    "    for x in X_trans:\n",
    "        x_tensor = torch.tensor(x, dtype=torch.float32, requires_grad=True).unsqueeze(0)\n",
    "        attr = ig.attribute(x_tensor, target=target_class)\n",
    "        attributions.append(attr.detach().numpy())\n",
    "\n",
    "    return np.vstack(attributions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "96536e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_attributions(attributions, feature_groups):\n",
    "    \"\"\"\n",
    "    attributions: np.ndarray of shape (n_samples, n_encoded_features)\n",
    "    feature_groups: dict {original_feature: [indices]}\n",
    "    \"\"\"\n",
    "    agg = np.zeros((attributions.shape[0], len(feature_groups)))\n",
    "    feature_names = list(feature_groups.keys())\n",
    "\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        idxs = feature_groups[feature]\n",
    "        agg[:, i] = np.abs(attributions[:, idxs]).sum(axis=1)\n",
    "\n",
    "    return agg, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4b41dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_groups(feature_names, colname, categorical_mask):\n",
    "    \"\"\"\n",
    "    Maps original features to lists of encoded feature indices.\n",
    "    Correctly handles categorical vs numerical features and avoids substring collisions.\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    # Build lookup sets\n",
    "    categorical_cols = {\n",
    "        colname[i] for i, is_cat in enumerate(categorical_mask) if is_cat\n",
    "    }\n",
    "    numerical_cols = {\n",
    "        colname[i] for i, is_cat in enumerate(categorical_mask) if not is_cat\n",
    "    }\n",
    "\n",
    "    for idx, name in enumerate(feature_names):\n",
    "        # Remove transformer prefix (e.g., \"num__\", \"cat__\")\n",
    "        clean = name.split(\"__\", 1)[-1]\n",
    "\n",
    "        # Case 1: numerical feature → exact match\n",
    "        if clean in numerical_cols:\n",
    "            original = clean\n",
    "\n",
    "        # Case 2: categorical feature → prefix before first \"_\" must match a categorical column\n",
    "        else:\n",
    "            base = clean.rsplit(\"_\", 1)[0]\n",
    "            if base in categorical_cols:\n",
    "                original = base\n",
    "            else:\n",
    "                raise ValueError(f\"Could not map encoded feature '{name}' to original column\")\n",
    "\n",
    "        groups[original].append(idx)\n",
    "\n",
    "    return dict(groups)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "ea3b7f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "7d18e13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = openml.datasets.get_dataset(dataset_id=DATASET_ID, download_data=True, download_qualities=True, download_features_meta_data=True)\n",
    "X, y, categorical_mask, colname=dataset.get_data(target=dataset.default_target_attribute , dataset_format=\"dataframe\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "col_names= X_train.columns.tolist()\n",
    "\n",
    "\n",
    "models = load_models(model_dir=\"models\", dataset_id=DATASET_ID)\n",
    "\n",
    "# Background & explanation sets\n",
    "X_background = X_train.sample(100, random_state=42)\n",
    "X_explain = X_test.iloc[:50]\n",
    "\n",
    "ig_mlp_vals = explain_with_ig(models[\"mlp\"], X_explain, target_class=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "e920daa2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 61)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ig_mlp_vals.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "d86ef2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_feature_names = models['lr'].named_steps['preprocess'].get_feature_names_out()\n",
    "feature_groups = build_feature_groups(encoded_feature_names, colname, categorical_mask)\n",
    "\n",
    "agg_ig_vals_mlp, agg_feature_names = aggregate_attributions(\n",
    "    ig_mlp_vals,\n",
    "    feature_groups\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "c37d6f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 20)"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_ig_vals_mlp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "966e003a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.12116869456600397,\n",
       " 0.10760753622700577,\n",
       " 0.14217530462308786,\n",
       " 0.07264229738211725,\n",
       " 0.18827516728197224,\n",
       " 0.11234481445979327,\n",
       " 0.0500178408192005,\n",
       " 0.3208823691890575,\n",
       " 0.22225476175546646,\n",
       " 0.26949433486908675,\n",
       " 0.1574879863858223,\n",
       " 0.15317084861919283,\n",
       " 0.13812534666154533,\n",
       " 0.11664594021160156,\n",
       " 0.15073125252034514,\n",
       " 0.2435778396576643,\n",
       " 0.14460015559569,\n",
       " 0.09166188139701262,\n",
       " 0.1527163844788447,\n",
       " 0.10831423468887806]"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agg_mean_ig_vals_mlp=[]\n",
    "\n",
    "for i in range(agg_ig_vals_mlp.shape[1]):\n",
    "    agg_mean_ig_vals_mlp.append(agg_ig_vals_mlp[:,i].mean())\n",
    "\n",
    "agg_mean_ig_vals_mlp"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv_IG",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
