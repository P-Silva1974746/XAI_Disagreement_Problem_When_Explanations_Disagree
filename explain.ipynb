{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6f3d094f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import openml\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import shap\n",
    "import lime\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2d93238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary to be here since the when loading the models joblid tries to import  __main__.to_float32_fn it wouldn't find it here\n",
    "# this happens because the MLP model pipeline depdends on this function\n",
    "def to_float32_fn(X):\n",
    "    return X.astype(np.float32)\n",
    "\n",
    "#same reason as above\n",
    "class MLP_IG(nn.Module):\n",
    "    def __init__(self, output_dim, hidden_dim=16):\n",
    "        super().__init__()\n",
    "\n",
    "        self.fc1 = nn.LazyLinear(hidden_dim, 16)\n",
    "        self.fc2 = nn.Linear(16, 16)\n",
    "        self.fc3 = nn.Linear(16, 16)\n",
    "        self.out = nn.Linear(16, output_dim)\n",
    "\n",
    "        # # better initialization for IG smoothness\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear) and not isinstance(m, nn.LazyLinear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        logits = self.out(x)  # no softmax\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d23fcdb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_models(model_dir, dataset_id):\n",
    "    models = {\n",
    "        \"lr\": joblib.load(f\"{model_dir}/lr_{dataset_id}.joblib\"),\n",
    "        \"rf\": joblib.load(f\"{model_dir}/rf_{dataset_id}.joblib\"),\n",
    "        \"mlp\": joblib.load(f\"{model_dir}/mlp_{dataset_id}.joblib\")\n",
    "    }\n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c3703ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_feature_names(pipeline):\n",
    "    preprocessor = pipeline.named_steps[\"preprocess\"]\n",
    "    return preprocessor.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d29476c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_lr(model, X_background, X_explain):\n",
    "    preprocessor = model.named_steps[\"preprocess\"]\n",
    "    lr_model = model.named_steps[\"clf\"]\n",
    "\n",
    "    X_bg_t = preprocessor.transform(X_background)\n",
    "    X_ex_t = preprocessor.transform(X_explain)\n",
    "\n",
    "    explainer = shap.LinearExplainer(\n",
    "        lr_model,\n",
    "        X_bg_t,\n",
    "        feature_perturbation=\"interventional\"\n",
    "    )\n",
    "    return explainer.shap_values(X_ex_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "17ca56cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_rf(model, X_explain):\n",
    "    preprocessor = model.named_steps[\"preprocess\"]\n",
    "    rf_model = model.named_steps[\"clf\"]\n",
    "\n",
    "    X_transformed = preprocessor.transform(X_explain)\n",
    "\n",
    "    # Shap can't handle sparse input so it is necessary to make it dense\n",
    "    if hasattr(X_transformed, \"toarray\"):\n",
    "        X_transformed = X_transformed.toarray()\n",
    "\n",
    "    # Shap also needs the input to numeric  Ensure numeric dtype\n",
    "    X_transformed = X_transformed.astype(np.float64)\n",
    "\n",
    "    explainer = shap.TreeExplainer(rf_model)\n",
    "    return explainer.shap_values(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e35dd0e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shap_mlp(model, X_background, X_explain, nsamples=100):\n",
    "    feature_names = X_background.columns\n",
    "\n",
    "    def predict_fn(x):\n",
    "        # KernelSHAP gives numpy arrays it is needed convert back to DataFrame so that our pipeline can use the ColumnTransformer\n",
    "        if isinstance(x, np.ndarray):\n",
    "            x = pd.DataFrame(x, columns=feature_names)\n",
    "        return model.predict_proba(x)\n",
    "\n",
    "    background = shap.sample(X_background, nsamples)\n",
    "\n",
    "    explainer = shap.KernelExplainer(\n",
    "        predict_fn,\n",
    "        background\n",
    "    )\n",
    "\n",
    "    shap_values = explainer.shap_values(X_explain.values, nsamples=nsamples)\n",
    "    return shap_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8cf0d0e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_categorical_imputer(model):\n",
    "    \"\"\"Creates a categorical Nan imputer\"\"\"\n",
    "    preprocess = model.named_steps[\"preprocess\"]\n",
    "    cat_transformer = dict(preprocess.named_transformers_)[\"cat\"]\n",
    "\n",
    "    # categories learned during fit\n",
    "    categories = cat_transformer.categories_\n",
    "\n",
    "    # use first category as a safe fill value\n",
    "    fill_values = {\n",
    "        col: cats[0]\n",
    "        for col, cats in zip(preprocess.transformers_[1][2], categories)\n",
    "    }\n",
    "    return fill_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "761ffff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_explainer(model, X_train):\n",
    "    preprocess = model.named_steps[\"preprocess\"]\n",
    "    X_train_trans = preprocess.transform(X_train)\n",
    "\n",
    "    if hasattr(X_train_trans, \"toarray\"):\n",
    "        X_train_trans = X_train_trans.toarray()\n",
    "\n",
    "    X_train_trans = X_train_trans.astype(np.float32)\n",
    "\n",
    "    feature_names = preprocess.get_feature_names_out()\n",
    "\n",
    "    explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "        training_data=X_train_trans,\n",
    "        feature_names=feature_names,\n",
    "        class_names=[\"class_0\", \"class_1\"],\n",
    "        mode=\"classification\",\n",
    "        discretize_continuous=False,\n",
    "        random_state=42\n",
    "    )\n",
    "    return explainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "11ad2183",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_explain_instance(explainer, model, x_instance, feature_names, cat_fill_values):\n",
    "\n",
    "    if cat_fill_values is not None:\n",
    "\n",
    "        preprocess = model.named_steps[\"preprocess\"]\n",
    "\n",
    "        x_df = pd.DataFrame([x_instance], columns=feature_names)\n",
    "\n",
    "        # manually impute categorical NaNs to fix fragile lime interaction with Nan values\n",
    "        for col, fill in cat_fill_values.items():\n",
    "            if col in x_df.columns:\n",
    "                x_df[col] = x_df[col].fillna(fill)\n",
    "\n",
    "        x_trans = preprocess.transform(x_df)\n",
    "\n",
    "        if hasattr(x_trans, \"toarray\"):\n",
    "            x_trans = x_trans.toarray()\n",
    "\n",
    "        x_trans = x_trans.astype(np.float32)\n",
    "\n",
    "        def predict_fn(x):\n",
    "            return model.named_steps[\"clf\"].predict_proba(x.astype(np.float32))\n",
    "\n",
    "        exp = explainer.explain_instance(\n",
    "            x_trans[0],\n",
    "            predict_fn,\n",
    "            num_features=x_trans.shape[1]\n",
    "        )\n",
    "\n",
    "        return exp.as_list()\n",
    "    \n",
    "    else:\n",
    "\n",
    "        preprocess = model.named_steps[\"preprocess\"] \n",
    "        x_df = pd.DataFrame([x_instance], columns=feature_names) \n",
    "        x_trans = preprocess.transform(x_df) \n",
    "        \n",
    "        if hasattr(x_trans, \"toarray\"): \n",
    "            x_trans = x_trans.toarray() \n",
    "        \n",
    "        def predict_fn(x): \n",
    "            x = x.astype(np.float32) \n",
    "            return model.named_steps[\"clf\"].predict_proba(x) \n",
    "        \n",
    "        exp = explainer.explain_instance(\n",
    "            x_trans[0], \n",
    "            predict_fn, \n",
    "            num_features=x_trans.shape[1], ) \n",
    "        \n",
    "        return exp.as_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9014b46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_feature_groups(feature_names, colname, categorical_mask):\n",
    "    \"\"\"\n",
    "    Maps original features to lists of encoded feature indices.\n",
    "    Correctly handles categorical vs numerical features and avoids substring collisions.\n",
    "    \"\"\"\n",
    "    groups = defaultdict(list)\n",
    "\n",
    "    # Build lookup sets\n",
    "    categorical_cols = {\n",
    "        colname[i] for i, is_cat in enumerate(categorical_mask) if is_cat\n",
    "    }\n",
    "    numerical_cols = {\n",
    "        colname[i] for i, is_cat in enumerate(categorical_mask) if not is_cat\n",
    "    }\n",
    "\n",
    "    for idx, name in enumerate(feature_names):\n",
    "        # Remove transformer prefix (e.g., \"num__\", \"cat__\")\n",
    "        clean = name.split(\"__\", 1)[-1]\n",
    "\n",
    "        # Case 1: numerical feature → exact match\n",
    "        if clean in numerical_cols:\n",
    "            original = clean\n",
    "\n",
    "        # Case 2: categorical feature → prefix before first \"_\" must match a categorical column\n",
    "        else:\n",
    "            base = clean.rsplit(\"_\", 1)[0]\n",
    "            if base in categorical_cols:\n",
    "                original = base\n",
    "            else:\n",
    "                raise ValueError(f\"Could not map encoded feature '{name}' to original column\")\n",
    "\n",
    "        groups[original].append(idx)\n",
    "\n",
    "    return dict(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2ca7a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_attributions(attributions, feature_groups):\n",
    "    \"\"\"\n",
    "    attributions: np.ndarray of shape (n_samples, n_encoded_features)\n",
    "    feature_groups: dict {original_feature: [indices]}\n",
    "    \"\"\"\n",
    "    agg = np.zeros((attributions.shape[0], len(feature_groups)))\n",
    "    feature_names = list(feature_groups.keys())\n",
    "\n",
    "    for i, feature in enumerate(feature_names):\n",
    "        idxs = feature_groups[feature]\n",
    "        agg[:, i] = np.abs(attributions[:, idxs]).sum(axis=1)\n",
    "\n",
    "    return agg, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f57c6e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lime_to_matrix(lime_exps, encoded_feature_names):\n",
    "    \n",
    "    n_samples = len(lime_exps)\n",
    "    n_features = len(encoded_feature_names)\n",
    "\n",
    "    mat = np.zeros((n_samples, n_features))\n",
    "    name_to_idx = {n: i for i, n in enumerate(encoded_feature_names)}\n",
    "\n",
    "    for i, lime_exp in enumerate(lime_exps):\n",
    "        for name, val in lime_exp:\n",
    "            # LIME uses \"feature=value\" or \"feature <= x\"\n",
    "            clean = (\n",
    "                name.replace(\"=\", \"_\")\n",
    "                    .replace(\"<=\", \"_\")\n",
    "                    .replace(\">\", \"_\")\n",
    "                    .replace(\" \", \"\")\n",
    "            )\n",
    "\n",
    "            if clean in name_to_idx:\n",
    "                mat[i, name_to_idx[clean]] = abs(val)\n",
    "\n",
    "    return mat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1796982e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = 1504#1590"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eaf370d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tLR Shap calculated\n",
      "\tRF Shap calculated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pedro/Documents/UNI/_4_ano/IAS/individual_project/.venv/lib/python3.12/site-packages/shap/explainers/_linear.py:99: FutureWarning: The feature_perturbation option is now deprecated in favor of using the appropriate masker (maskers.Independent, maskers.Partition or maskers.Impute).\n",
      "  warnings.warn(wmsg, FutureWarning)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47714f574b814ab4974abad02432dacd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tMLP Shap calculated\n",
      "Lime calulated for instance: 0\n",
      "Lime calulated for instance: 5\n",
      "Lime calulated for instance: 10\n",
      "Lime calulated for instance: 15\n",
      "Lime calulated for instance: 20\n",
      "Lime calulated for instance: 25\n",
      "Lime calulated for instance: 30\n",
      "Lime calulated for instance: 35\n",
      "Lime calulated for instance: 40\n",
      "Lime calulated for instance: 45\n",
      "\tLR Lime calculated\n",
      "\tRF Lime calculated\n",
      "\tMLP Lime calculated\n"
     ]
    }
   ],
   "source": [
    "dataset = openml.datasets.get_dataset(dataset_id=DATASET_ID, download_data=True, download_qualities=True, download_features_meta_data=True)\n",
    "X, y, categorical_mask, colname=dataset.get_data(target=dataset.default_target_attribute , dataset_format=\"dataframe\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 42)\n",
    "col_names= X_train.columns.tolist()\n",
    "\n",
    "# Drop raw string columns\n",
    "# string_cols = X.select_dtypes(include=[\"object\"]).columns.tolist()\n",
    "# if string_cols:\n",
    "#     print(f\"Dropping string columns in dataset {id}: {string_cols}\")\n",
    "#     X = X.drop(columns=string_cols)\n",
    "\n",
    "models = load_models(model_dir=\"models\", dataset_id=DATASET_ID)\n",
    "\n",
    "# Background & explanation sets\n",
    "X_background = X_train.sample(100, random_state=42)\n",
    "X_explain = X_test.iloc[:50]\n",
    "\n",
    "# print(f\"Starting the explanations for dataset {dataset.name} (id: {DATASET_ID})\")\n",
    "\n",
    "# SHAP\n",
    "shap_lr_vals = shap_lr(models[\"lr\"], X_background, X_explain)\n",
    "print(\"\\tLR Shap calculated\")\n",
    "shap_rf_vals = shap_rf(models[\"rf\"], X_explain)\n",
    "print(\"\\tRF Shap calculated\")\n",
    "shap_mlp_vals = shap_mlp(models[\"mlp\"], X_background, X_explain)\n",
    "print(\"\\tMLP Shap calculated\")\n",
    "\n",
    "\n",
    "lime_lr_vals=[]\n",
    "lime_rf_vals=[]\n",
    "lime_mlp_vals=[]\n",
    "\n",
    "# LIME\n",
    "lime_exp = lime_explainer(\n",
    "    model=models[\"lr\"],\n",
    "    X_train=X_train,\n",
    ")\n",
    "\n",
    "cat_fill_values = None\n",
    "for val in categorical_mask:\n",
    "    if val:\n",
    "        cat_fill_values = build_categorical_imputer(models[\"lr\"])\n",
    "        break\n",
    "\n",
    "for i in range(50):\n",
    "    try:\n",
    "        lime_lr_vals.append(lime_explain_instance(\n",
    "            lime_exp,\n",
    "            models[\"lr\"],\n",
    "            X_explain.iloc[i].values,\n",
    "            col_names,\n",
    "            cat_fill_values\n",
    "        ))\n",
    "        if i==49:\n",
    "            print(\"\\tLR Lime calculated\")\n",
    "\n",
    "        lime_rf_vals.append(lime_explain_instance(\n",
    "            lime_exp,\n",
    "            models[\"rf\"],\n",
    "            X_explain.iloc[i].values,\n",
    "            col_names,\n",
    "            cat_fill_values\n",
    "        ))\n",
    "        if i==49:\n",
    "            print(\"\\tRF Lime calculated\")\n",
    "\n",
    "        lime_mlp_vals.append(lime_explain_instance(\n",
    "            lime_exp,\n",
    "            models[\"mlp\"],\n",
    "            X_explain.iloc[i].values,\n",
    "            col_names,\n",
    "            cat_fill_values\n",
    "        ))\n",
    "        if i==49:\n",
    "            print(\"\\tMLP Lime calculated\")\n",
    "        if i%5==0:\n",
    "            print(f\"Lime calulated for instance: {i}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Instance: {i}\")\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "36d4e4f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression aggregated shap values calculated new shape: (50, 33)\n",
      "Logistic Regression aggregated lime values calculated new shape: (50, 33)\n",
      "Random Forest aggregated shap values calculated new shape: (50, 33)\n",
      "Random Fores aggregated lime values calculated new shape: (50, 33)\n",
      "MLP aggregated shap values calculated new shape: (50, 33)\n",
      "MLP aggregated lime values calculated new shape: (50, 33)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Logistic Regression\n",
    "encoded_feature_names = get_feature_names(pipeline=models['lr'])\n",
    "feature_groups = build_feature_groups(encoded_feature_names, colname, categorical_mask)\n",
    "\n",
    "agg_shap_vals_lr, agg_feature_names = aggregate_attributions(\n",
    "    shap_lr_vals,\n",
    "    feature_groups\n",
    ")\n",
    "print(f\"Logistic Regression aggregated shap values calculated new shape: {agg_shap_vals_lr.shape}\")\n",
    "\n",
    "lime_lr_matrix = lime_to_matrix(lime_lr_vals, encoded_feature_names)\n",
    "agg_lime_lr_vals, agg_feature_names = aggregate_attributions(\n",
    "    lime_lr_matrix,\n",
    "    feature_groups\n",
    ")\n",
    "print(f\"Logistic Regression aggregated lime values calculated new shape: {agg_lime_lr_vals.shape}\")\n",
    "\n",
    "# Random Forest\n",
    "\n",
    "encoded_feature_names = get_feature_names(pipeline=models['rf'])\n",
    "feature_groups = build_feature_groups(encoded_feature_names, colname, categorical_mask)\n",
    "\n",
    "agg_shap_vals_rf, agg_feature_names = aggregate_attributions(\n",
    "    shap_rf_vals[:,:,1], #only class one shap values\n",
    "    feature_groups\n",
    ")\n",
    "print(f\"Random Forest aggregated shap values calculated new shape: {agg_shap_vals_rf.shape}\")\n",
    "\n",
    "\n",
    "lime_rf_matrix = lime_to_matrix(lime_rf_vals, encoded_feature_names)\n",
    "agg_lime_rf_vals, agg_feature_names = aggregate_attributions(\n",
    "    lime_rf_matrix,\n",
    "    feature_groups\n",
    ")\n",
    "print(f\"Random Fores aggregated lime values calculated new shape: {agg_lime_rf_vals.shape}\")\n",
    "\n",
    "\n",
    "# MLP\n",
    "agg_shap_vals_mlp = np.abs(shap_mlp_vals[:, :, 1])\n",
    "agg_feature_names = X_explain.columns.tolist()\n",
    "print(f\"MLP aggregated shap values calculated new shape: {agg_shap_vals_mlp.shape}\")\n",
    "\n",
    "lime_mlp_matrix = lime_to_matrix(lime_mlp_vals, encoded_feature_names)\n",
    "agg_lime_mlp_vals, agg_feature_names = aggregate_attributions(\n",
    "    lime_mlp_matrix,\n",
    "    feature_groups\n",
    ")\n",
    "print(f\"MLP aggregated lime values calculated new shape: {agg_lime_mlp_vals.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e32e57a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.4598001827366471\n",
      "0.22738524012292122\n",
      "0.2639523842977689\n",
      "\n",
      "0.20112752944413137\n",
      "0.21388689874965686\n",
      "0.22840329008397525\n"
     ]
    }
   ],
   "source": [
    "agg_mean_shap_vals_lr=[]\n",
    "agg_mean_shap_vals_rf=[]\n",
    "agg_mean_shap_vals_mlp=[]\n",
    "\n",
    "agg_mean_lime_vals_lr=[]\n",
    "agg_mean_lime_vals_rf=[]\n",
    "agg_mean_lime_vals_mlp=[]\n",
    "\n",
    "for i in range(agg_shap_vals_lr.shape[1]):\n",
    "    agg_mean_shap_vals_lr.append(agg_shap_vals_lr[:,i].mean())\n",
    "    agg_mean_shap_vals_rf.append(agg_shap_vals_rf[:,i].mean())\n",
    "    agg_mean_shap_vals_mlp.append(agg_shap_vals_mlp[:,i].mean())\n",
    "\n",
    "    agg_mean_lime_vals_lr.append(agg_lime_lr_vals[:,i].mean())\n",
    "    agg_mean_lime_vals_rf.append(agg_lime_rf_vals[:,i].mean())\n",
    "    agg_mean_lime_vals_mlp.append(agg_lime_mlp_vals[:,i].mean())\n",
    "\n",
    "print(agg_mean_shap_vals_lr[-1])\n",
    "print(agg_mean_shap_vals_rf[-1])\n",
    "print(agg_mean_shap_vals_mlp[-1])\n",
    "print(\"\")\n",
    "print(agg_mean_lime_vals_lr[-1])\n",
    "print(agg_mean_lime_vals_rf[-1])\n",
    "print(agg_mean_lime_vals_mlp[-1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
